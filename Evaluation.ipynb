{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528bc198",
   "metadata": {},
   "source": [
    "# Installing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU pandas fuzzywuzzy rouge sentence-transformers scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61dff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from rouge import Rouge\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0361a62",
   "metadata": {},
   "source": [
    "# Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09666bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_coverage(fmea_df):\n",
    "    \"\"\"\n",
    "    Evaluates the FMEA dataframe for completeness and consistency.\n",
    "    \"\"\"\n",
    "    # Check for missing values in critical columns\n",
    "    missing_effects = fmea_df['Potential Effects'].isnull().sum()\n",
    "    missing_causes = fmea_df['Potential Causes'].isnull().sum()\n",
    "    missing_actions = fmea_df['Recommended Actions'].isnull().sum()\n",
    "    missing_effects_attr = fmea_df['Effects_Attr'].isnull().sum()\n",
    "    missing_causes_attr = fmea_df['Causes_Attr'].isnull().sum()\n",
    "    missing_actions_attr = fmea_df['Recommended_Actions_Attr'].isnull().sum()\n",
    "\n",
    "    # Total rows\n",
    "    total_rows = len(fmea_df)\n",
    "\n",
    "    # Completeness metrics\n",
    "    completeness = {\n",
    "        \"Total Rows\": total_rows,\n",
    "        \"Missing Effects\": missing_effects,\n",
    "        \"Missing Causes\": missing_causes,\n",
    "        \"Missing Actions\": missing_actions,\n",
    "        \"Missing Effects Attribution\": missing_effects_attr,\n",
    "        \"Missing Causes Attribution\": missing_causes_attr,\n",
    "        \"Missing Actions Attribution\": missing_actions_attr,\n",
    "    }\n",
    "\n",
    "    # Consistency check: Ensure no duplicate (Process, Failure Mode) pairs\n",
    "    duplicate_pairs = fmea_df.duplicated(subset=['SMT Process', 'Failure Mode']).sum()\n",
    "\n",
    "    # Consistency metrics\n",
    "    consistency = {\n",
    "        \"Duplicate Pairs\": duplicate_pairs,\n",
    "    }\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(\"FMEA Evaluation Results:\")\n",
    "    print(\"Completeness:\")\n",
    "    for key, value in completeness.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    print(\"\\nConsistency:\")\n",
    "    for key, value in consistency.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    return {\"completeness\": completeness, \"consistency\": consistency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_fmea_df = pd.read_csv('/content/Final Generated FMEA.csv')\n",
    "evaluation_results = evaluate_coverage(generated_fmea_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a48d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = pd.read_csv('/content/Ground Truth FMEA.csv')\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a986b66",
   "metadata": {},
   "source": [
    "# Quantitative Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a709a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    return str(text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b091b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_generation_metrics(generated_text, ground_truth_text):\n",
    "    \"\"\"Calculate BLEU, ROUGE, and Semantic Similarity for generated text.\"\"\"\n",
    "    # BLEU Score with smoothing\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu([ground_truth_text.split()], generated_text.split(), smoothing_function=smoothing)\n",
    "\n",
    "    # ROUGE Score\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_text, ground_truth_text)[0]\n",
    "\n",
    "    # Semantic Similarity\n",
    "    embeddings = semantic_model.encode([generated_text, ground_truth_text])\n",
    "    semantic_similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-L\": rouge_scores[\"rouge-l\"][\"f\"],\n",
    "        \"Semantic Similarity\": semantic_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09817a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fmea_metrics(ground_truth_df, generated_fmea_df):\n",
    "    metrics = {\n",
    "        \"generation\": {\n",
    "            \"Potential Effects\": [],\n",
    "            \"Potential Causes\": [],\n",
    "            \"Recommended Actions\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for _, row in generated_fmea_df.iterrows():\n",
    "        # Match ground truth row\n",
    "        gt_row = ground_truth_df[\n",
    "            (ground_truth_df[\"SMT Process\"] == row[\"SMT Process\"]) &\n",
    "            (ground_truth_df[\"Failure Mode\"] == row[\"Failure Mode\"])\n",
    "        ]\n",
    "\n",
    "        if not gt_row.empty:\n",
    "\n",
    "            # Generation Metrics\n",
    "            for col in [\"Potential Effects\", \"Potential Causes\", \"Recommended Actions\"]:\n",
    "                generated_text = normalize_text(row[col])\n",
    "                ground_truth_text = normalize_text(gt_row.iloc[0][col])\n",
    "                generation_metrics = calculate_generation_metrics(generated_text, ground_truth_text)\n",
    "                metrics[\"generation\"][col].append(generation_metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb43957",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_fmea_metrics(ground_truth_df, generated_fmea_df)\n",
    "\n",
    "# Print results\n",
    "for col, col_metrics in metrics[\"generation\"].items():\n",
    "    print(f\"Metrics for {col}:\")\n",
    "    avg_bleu = sum(m[\"BLEU\"] for m in col_metrics) / len(col_metrics)\n",
    "    avg_rouge = sum(m[\"ROUGE-L\"] for m in col_metrics) / len(col_metrics)\n",
    "    avg_semantic = sum(m[\"Semantic Similarity\"] for m in col_metrics) / len(col_metrics)\n",
    "    print(f\"  Average BLEU: {avg_bleu:.2f}\")\n",
    "    print(f\"  Average ROUGE-L: {avg_rouge:.2f}\")\n",
    "    print(f\"  Average Semantic Similarity: {avg_semantic:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
